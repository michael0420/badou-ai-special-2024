import warnings
import numpy as np

from keras.preprocessing import image

from keras.models import Model
from keras.layers import DepthwiseConv2D,Input,Activation,Dropout,Reshape,BatchNormalization,GlobalAveragePooling2D,GlobalMaxPooling2D,Conv2D
from keras.applications.imagenet_utils import decode_predictions
from keras import backend as K

def relu6(x):
    return K.relu(x, max_value=6)    #relu函数的阈值设成6，最大值为6

def _conv_block(inputs, filters, kernel=(3, 3), strides=(1, 1)):
    x = Conv2D(filters, kernel,
               padding='same',
               use_bias=False,
               strides=strides,
               name='conv1')(inputs)
    x = BatchNormalization(name='conv1_bn')(x)
    return Activation(relu6, name='conv1_relu')(x)

def _depthwise_conv_block(inputs, pointwise_conv_filters,depth_multiplier=1, strides=(1, 1), block_id=1):
    x=DepthwiseConv2D((3,3),padding="same",depth_multiplier=depth_multiplier,strides=strides,use_bias=False,name="conv_dw_%d"%(block_id))(inputs)
    x=BatchNormalization(name="conv_dw_%d_bn"%(block_id))(x)
    x=Activation(relu6, name='conv_dw_%d_relu' % block_id)(x)
    #Conv2D用来改变通道数
    x=Conv2D(pointwise_conv_filters,(1,1),padding="same",strides=(1,1),use_bias=False,name='conv_pw_%d' % block_id)(x)
    x=BatchNormalization(name='conv_pw_%d_bn' % block_id)(x)
    x=Activation(relu6,name='conv_pw_%d_relu' % block_id)(x)
    return x


def MobileNet(input_shape=[224,224,3],
              depth_multiplier=1,
              dropout=1e-3,
              classes=1000):
    img_input=Input(shape=input_shape)           #Input是方法而不是类，因此，img_input是一个张量，在之后的model中传入
    # 224,224,3 -> 112,112,32
    x = _conv_block(img_input, 32, strides=(2, 2))

    # 112,112,32 -> 112,112,64
    x = _depthwise_conv_block(x, 64, depth_multiplier, block_id=1)

    # 112,112,64 -> 56,56,128
    x = _depthwise_conv_block(x, 128, depth_multiplier,strides=(2, 2), block_id=2)

    # 56,56,128 -> 56,56,128
    x = _depthwise_conv_block(x, 128, depth_multiplier, block_id=3)

    # 56,56,128 -> 28,28,256
    x = _depthwise_conv_block(x, 256, depth_multiplier,strides=(2, 2), block_id=4)

    # 28,28,256 -> 28,28,256
    x = _depthwise_conv_block(x, 256, depth_multiplier, block_id=5)

    # 28,28,256 -> 14,14,512
    x = _depthwise_conv_block(x, 512, depth_multiplier,strides=(2, 2), block_id=6)

    # 14,14,512 -> 14,14,512
    x = _depthwise_conv_block(x, 512, depth_multiplier, block_id=7)
    x = _depthwise_conv_block(x, 512, depth_multiplier, block_id=8)
    x = _depthwise_conv_block(x, 512, depth_multiplier, block_id=9)
    x = _depthwise_conv_block(x, 512, depth_multiplier, block_id=10)
    x = _depthwise_conv_block(x, 512, depth_multiplier, block_id=11)

    # 14,14,512 -> 7,7,1024
    x = _depthwise_conv_block(x, 1024, depth_multiplier,strides=(2, 2), block_id=12)
    # 7,7,1024 -> 7,7,1024
    x = _depthwise_conv_block(x, 1024, depth_multiplier, block_id=13)

    # 7,7,1024 -> 1,1,1024
    x = GlobalAveragePooling2D()(x)                   #GlobalAveragePooling2D之后的x只有两个维度了，batch_size和通道数
    x = Reshape((1, 1, 1024), name='reshape_1')(x)    #把最后一层变成1*1*1024,不需要在目标形状中指定 batch_size
    x = Dropout(dropout, name='dropout')(x)
    x = Conv2D(classes, (1, 1), padding='same', name='conv_preds')(x)   #用卷积代替全连接
    x = Activation('softmax', name='act_softmax')(x)
    x = Reshape((classes,), name='reshape_2')(x)        #(classes,)表示一个形状,而(classes)是一个普通的整数，表示数字 classes，而不是元组。在 Python 中，使用单个值的括号不会创建元组。

    inputs = img_input
    model=Model(inputs,x,name='mobilenet_1_0_224_tf')
    model_name = 'mobilenet_1_0_224_tf.h5'
    model.load_weights(model_name)
    return model


def preprocess_input(x):
    x /= 255.
    x -= 0.5
    x *= 2.
    return x

if __name__ == '__main__':
    model = MobileNet(input_shape=(224, 224, 3))
    img_path = 'elephant.jpg'
    img=image.load_img(img_path,target_size=(224,224))      #返回的是一个 PIL 图像对象。
    x=image.img_to_array(img)
    x=np.expand_dims(x,axis=0)
    x=preprocess_input(x)

    preds = model.predict(x)
    print(np.argmax(preds))
    print('Predicted:', decode_predictions(preds, 1))  # 只显示top1
